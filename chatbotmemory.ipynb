{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-0.2.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
      "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langchain_groq) (0.3.34)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (0.3.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_groq) (24.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_groq) (2.3.0)\n",
      "Downloading langchain_groq-0.2.4-py3-none-any.whl (14 kB)\n",
      "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: groq, langchain_groq\n",
      "Successfully installed groq-0.18.0 langchain_groq-0.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install  langchain_groq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000238006120D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000238006202D0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\")\n",
    "\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! 👋\\n\\nHow can I help you today? 😊\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 10, 'total_tokens': 25, 'completion_time': 0.027272727, 'prompt_time': 4e-07, 'queue_time': 0.021345985999999997, 'total_time': 0.027273127}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-b008a2ab-8e2b-4124-9eb1-a3ac4b88a9c0-0', usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "   \"Hello\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Krish, and you are a Chief AI Engineer! \\n\\nIs there anything else I can help you with? 😊  Perhaps you'd like to discuss some AI projects or explore the latest advancements in the field?  I'm here to chat. \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 75, 'total_tokens': 132, 'completion_time': 0.103636364, 'prompt_time': 0.002081715, 'queue_time': 0.02227417, 'total_time': 0.105718079}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-6ecd44d5-5b9e-48ac-9156-738afd7802a8-0', usage_metadata={'input_tokens': 75, 'output_tokens': 57, 'total_tokens': 132})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_core.messages \n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Krish and I am a Chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Krish! It's nice to meet you. \\n\\nAs a Chief AI Engineer, what kind of projects are you working on these days?\\n\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id)-> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x0000023811FC3EC0>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    model,\n",
    "    get_session_history\n",
    ")\n",
    "\n",
    "\n",
    "with_message_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and no access to personal information about you. So, I don't know your name or what you do.\\n\\nIf you'd like to tell me, I'm happy to learn! 😊\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 21, 'total_tokens': 76, 'completion_time': 0.1, 'prompt_time': 0.00015237, 'queue_time': 0.021007055, 'total_time': 0.10015237}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-69c30d12-e228-4ea9-884d-ee0a9b79dd28-0', usage_metadata={'input_tokens': 21, 'output_tokens': 55, 'total_tokens': 76})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"krish\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    HumanMessage(content=\"Hey What's my name and what do I do?\"),\n",
    "    config = config\n",
    ")\n",
    "\n",
    "response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Since I don't have memory of past conversations, I don't know your name or what you do.  \\n\\nIs there anything else I can help you with? 😊 \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 160, 'total_tokens': 201, 'completion_time': 0.074545455, 'prompt_time': 0.004926121, 'queue_time': 0.022353343999999997, 'total_time': 0.079471576}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d37fb021-b0c8-4d92-95a5-167b92f619c3-0', usage_metadata={'input_tokens': 160, 'output_tokens': 41, 'total_tokens': 201})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1 = {\"configurable\": {\"session_id\": \"krish\"}}\n",
    "\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    HumanMessage(content=\"Hey What's my name?\"),\n",
    "    config= config\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Since I don't have access to past conversations, I don't know your name.  \\n\\nWhat is your name? 😊  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 71, 'total_tokens': 102, 'completion_time': 0.056363636, 'prompt_time': 0.002141153, 'queue_time': 0.021647615, 'total_time': 0.058504789}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-d92cae22-41c4-440a-a33e-5fc93ef6a1cd-0', usage_metadata={'input_tokens': 71, 'output_tokens': 31, 'total_tokens': 102})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = {\"configurable\": {\"session_id\": \"John\"}}\n",
    "\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    HumanMessage(content=\"Hey What's my name?\"),\n",
    "    config= config2\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \n",
    "         \"You are a helpful assistant answering questions about the user in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to ChatPromptTemplate is missing variables {'language'}.  Expected: ['language', 'messages'] Received: ['messages']\\nNote: if you intended {language} to be part of the string and not a variable, please escape it with double curly braces like: '{{language}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHey my name is Krish and I am a Chief AI Engineer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\My-Pc\\Desktop\\langchain\\genai\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3014\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3012\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   3013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3014\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3016\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\My-Pc\\Desktop\\langchain\\genai\\myenv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:210\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[0;32m    209\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\My-Pc\\Desktop\\langchain\\genai\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1914\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1910\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1911\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1912\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1913\u001b[0m         Output,\n\u001b[1;32m-> 1914\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1917\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1918\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1922\u001b[0m     )\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1924\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\My-Pc\\Desktop\\langchain\\genai\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\My-Pc\\Desktop\\langchain\\genai\\myenv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:184\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m--> 184\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[1;32mc:\\Users\\My-Pc\\Desktop\\langchain\\genai\\myenv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:178\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    172\u001b[0m     example_key \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    173\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m to be part of the string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m     )\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m         create_message(message\u001b[38;5;241m=\u001b[39mmsg, error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_PROMPT_INPUT)\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Input to ChatPromptTemplate is missing variables {'language'}.  Expected: ['language', 'messages'] Received: ['messages']\\nNote: if you intended {language} to be part of the string and not a variable, please escape it with double curly braces like: '{{language}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hey my name is Krish and I am a Chief AI Engineer\")]}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm designed to be helpful and provide information, but I don't have access to any personal information about you, including your name.  \\n\\nIf you'd like to tell me your name, I'm happy to remember it for our current conversation! 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 233, 'total_tokens': 293, 'completion_time': 0.109090909, 'prompt_time': 0.011772954, 'queue_time': 0.022231482999999996, 'total_time': 0.120863863}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f656ad01-89a3-4e8b-a4e0-7dad88edc9e7-0', usage_metadata={'input_tokens': 233, 'output_tokens': 60, 'total_tokens': 293})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    HumanMessage(content=\"Hey What's my name?\"),\n",
    "    config= config\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Please tell me more about what you'd like to know! I need a bit more information to be helpful.  \\n\\nFor example, are you asking:\\n\\n* **Who is Hey name in Nernay?** \\n* **What is Hey name known for in Nernay?**\\n* **Is there anything specific about Hey name in Nernay that you'd like to learn?**\\n\\n\\nThe more details you give me, the better I can assist you! 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 26, 'total_tokens': 131, 'completion_time': 0.190909091, 'prompt_time': 0.001626845, 'queue_time': 0.022582942, 'total_time': 0.192535936}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-bc453a96-362b-4db6-a9d9-443a083600cb-0', usage_metadata={'input_tokens': 26, 'output_tokens': 105, 'total_tokens': 131})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hey name in Nernay?\")],\n",
    "    \"language\": \"hindi\"\n",
    "})\n",
    "\n",
    "response\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\my-pc\\desktop\\langchain\\genai\\myenv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.1/9.7 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.7 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.3/9.7 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.7 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 15.9 MB/s eta 0:00:00\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.17.0 fsspec-2025.2.0 huggingface-hub-0.28.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\My-Pc\\Desktop\\langchain\\genai\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=60,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you like vanilla ice cream! 🍦  \\n\\nIs there anything else I can help you with? 😊\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=( RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)| prompt | model )\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
